from multiprocessing import Pool, cpu_count
import pypi_ssnmf
import numpy as np
import pandas as pd
import itertools

from pypi_ssnmf import Pypi_SSNMF
from ssnmf_utils import *

from sklearn.model_selection import train_test_split as sk_train_test_split
from sklearn.model_selection import KFold

from scipy.optimize import lsq_linear
from sklearn.metrics import accuracy_score
from collections import namedtuple


'''
These functions implement multiprocessing for a list of SSNMF experiments, provided as a list of tuples (data, labels).
The main function is haddock_multi_ssnmf; it returns a namedtuple Haddock_Multi_SSNMF(gridsearch_results, train_results, test_results).

'''

# PARAM_RANGE = {'k': range(2,7),'lambda': list(np.linspace(0,1,10)), 'random_state': range(0,10)}
PARAM_RANGE = {'k': [4],'lambda': [1], 'random_state': [0]}

Haddock_Multi_SSNMF = namedtuple('Haddock_Multi_SSNMF', ['gridsearch_results','train_results','test_results'])

def get_Xreconerr(model):
    '''
    Given a trained model, compute reconstruction error ||X-AS||_F,
    using frobenius norm.

    Parameters:
    model (Pypi_SSNMF): trained model

    Return:
    reconerr (float): reconstruction error
    '''
    
    return np.linalg.norm(np.multiply(model.W, model.X - model.A @ model.S), ord='fro')

def get_Yreconerr(model):
    '''
    Given a trained model, compute reconstruction error ||Y-BS||_F,
    using frobenius norm.

    Parameters:
    model (Pypi_SSNMF): trained model

    Return:
    reconerr (float): reconstruction error
    '''
    
    return np.linalg.norm(np.multiply(model.L, model.Y - model.B @ model.S), ord='fro')

def find_matrix_S(X, A, **kwargs):
    '''
    Compute coefficient matrix S from data matrix X and basis matrix A using 
    Nonnegative Least Squares: X approx. AS.
    
    Parameters:
    X (np.array): data matrix, cases x features
    A (np.array): basis matrix, cases x topics
    W (np.array, optional): weight matrix, cases x features
    
    Returns:
    S (np.array): coefficient matrix, topics x features
    tol (float, optional): NNLS tolerance, default 1e-10
    '''
    if X.shape[0] != A.shape[0]:
        raise Exception('Shape mismatch, X: ',X.shape,' A: ',A.shape)
    
    W = kwargs.get('W', np.ones(X.shape))
    tol = kwargs.get('tol',1e-10)
    
    num_samples, num_features = X.shape
    num_components = A.shape[1]
    
    S = np.zeros((num_components, num_features))
    ### Decrease precision to avoid NNLS Non-convergence? Or reduce # of iterations
    
    for i in range(num_features):
        if W is None:
            A_check = A
            X_check = X[:, i]
        else:
            W_i = W[:, i]
            W_i_matrix = np.diag(W_i)
            A_check = W_i_matrix @ A
            X_check = W_i_matrix @ X[:, i]

        if np.isnan(A_check).any() or np.isinf(A_check).any():
            print(f"NaN or Inf detected in weighted A matrix at feature {i}")
        if np.isnan(X_check).any() or np.isinf(X_check).any():
            print(f"NaN or Inf detected in weighted X vector at feature {i}")
        if np.isnan(W_i).any() or np.isinf(W_i).any():
            print(f"NaN or Inf detected in weight vector at feature {i}")

        # clip very small weights to avoid zeros:
        if W is not None:
            W_i = np.clip(W_i, 1e-10, None)  # avoid zeros
            W_i_matrix = np.diag(W_i)
            A_check = W_i_matrix @ A
            X_check = W_i_matrix @ X[:, i]
            
        nnls_result = lsq_linear(A_check, X_check, bounds=(0, np.inf), method='bvls', tol=tol)
        s_i = nnls_result.x
        S[:, i] = s_i
    return S

def get_accuracy(model, X_data, Y_labels, **kwargs):
    '''
    Given a trained model and actual label matrix,
    calculate accuracy by comparing the predicted labels to the 
    actual labels.
    
    Parameters:
    model (Pypi_SSNMF): trained model
    X_data (np.array): data to predict labels
    Y_labels (np.array): actual label matrix (rows=patients, col=label)
    S (np.array, optional): coefficient matrix, specified for fulldata_validation or calculated via NNLS if unspecified.
    A (np.array): matrix A
    W (np.array, optional): weight matrix W
    
    Returns:
    accuracy (float): accuracy score
    X_tst_err (float): ||X - AS|| for S generated by NNLS
    '''
    A = kwargs.get('A', model.A)
    B = model.B
   
    W = kwargs.get('W', np.ones(X_data.shape))
    
    S = kwargs.get('S', find_matrix_S(X_data, A, W=W))
    X_tst_err = np.linalg.norm(X_data - A@S, ord='fro')
    
    Y_hat = B @ S

    if Y_labels.ndim == 1:
        Y_labels = np.column_stack((Y_labels, 1-Y_labels)) # Make Y_labels 2D

    Y_labels = Y_labels.T
    Y_hat = Y_hat.T        
    y_len = len(Y_hat)
    
    Y_hat_labels = None

    num_labels = Y_labels.shape[1]
    
    if num_labels == 2 and (np.any(np.all(Y_labels == 1, axis=1)) or np.any(np.all(Y_labels == 0, axis=1))):
        Y_hat_labels = np.round(Y_hat)
    else:
        result = np.zeros(Y_hat.shape)
        max_indices = np.argmax(Y_hat, axis=1)
        result[np.arange(y_len), max_indices] = 1
        Y_hat_labels = result
    assert Y_hat_labels.shape == Y_labels.shape
        
    correct_pred = 0 # True positive + True negative
    
    correct_pred = np.sum(np.all(Y_labels == Y_hat_labels, axis=1))
    
    accuracy = correct_pred / y_len # (TP + TN)/(TP+TN+FP+FN)
    return accuracy, X_tst_err
    
def cross_validate(param_vals, experiment, kf, **kwargs):

    '''
    Compute the accuracy using cross-validation on Pypi_SSNMF
    with specified parameters.
    
    Parameters:
    kf (KFold): KFold object to split training data into folds
    param_vals (namedtuple): Param(k=, lambda=, random_state=)
    N (int, optional): number of iterations to train SSNMF, default 1000
    
    
    Returns:
    Crossvalidation_Result (namedtuple): 'validation_score','param_vals','xreconerr','yreconerr','x_valreconerr'
    
    validation_score (float): accuracy score, averaged over all folds
    param_vals (namedtuple): Param(k, lambda, random_state)
    xreconerr (float): ||X-AS|| reconstruction error, averaged over all folds
    yreconerr (float): ||X-AS|| reconstruction error, averaged over all folds
    x_valreconerr (float): ||X_cv_tst -A S_nnls|| reconstruction error, averaged over all folds
    '''
    
    N = kwargs.get('N', 1000)
    
    X_train, Y_train, X_test, Y_test, W_train, W_test = experiment.X_train, experiment.Y_train, experiment.X_test, experiment.Y_test, experiment.W_train, experiment.W_test

    scores = []
    X_errs = []
    Y_errs = []
    X_tst_errs = []
    
    for train_index, val_index in kf.split(X_train):
        X_train_cv, X_val_cv = X_train[train_index, :].T, X_train[val_index, :].T
        Y_train_cv, Y_val_cv = Y_train[train_index, :].T, Y_train[val_index, :].T
        W_train_cv, W_val_cv = W_train[train_index, :].T, W_train[val_index, :].T

        # X_train_cv: features x patients
        # Y_train_cv: features x labels
            
        model = Pypi_SSNMF(X=X_train_cv, Y=Y_train_cv, W=W_train_cv, k=param_vals.k, lam=param_vals.lambda_val, random_state=param_vals.random_state, modelNum=3)
        
        
        model.mult(numiters=N)

        accuracy, X_tst_err = get_accuracy(model,X_val_cv,Y_val_cv) 
        scores.append(accuracy)
        X_tst_errs.append(X_tst_err)

        X_err = get_Xreconerr(model)
        X_errs.append(X_err)

        Y_err = get_Yreconerr(model)
        Y_errs.append(Y_err)
        
    avg_score = np.mean(scores)
    avg_X_reconerr = np.mean(X_errs)
    avg_Y_reconerr = np.mean(Y_errs)
    avg_X_tst_err = np.mean(X_tst_errs)

    return Crossvalidation_Result(avg_score, param_vals, avg_X_reconerr, avg_Y_reconerr, avg_X_tst_err)


def test(param_vals, experiment, *args, **kwargs):
    '''
    Using provided parameter values calculated by gridsearch or specified values, train SSNMF on full training set, set self.best_model, then tests it.
    
    Parameters:
    param_vals (namedtuple): Param(k=, lambda_val=, random_state=)
    experiment (dict): dictionary of numpy arrays X_train, X_test, etc.
    
    Returns:
    accuracy (float): calculated by sklearn.accuracy_score
    X_tst_err (float): ||X_tst - A S_nnls ||
    '''
    N = kwargs.get('N', 1000)
    
    X_train, Y_train, X_test, Y_test, W_train, W_test = experiment.X_train, experiment.Y_train, experiment.X_test, experiment.Y_test, experiment.W_train, experiment.W_test
    if param_vals is None:
        raise Exception('Call Gridsearch before testing!')
    
    W_train = np.ones(X_train.shape)
        
    best_model = Pypi_SSNMF(X=X_train.T, Y=Y_train.T, W=W_train.T, k=param_vals.k, lam=param_vals.lambda_val, random_state=param_vals.random_state, modelNum=3)
    
    best_model.mult(numiters=N)

    test_accuracy, x_tst_err = get_accuracy(best_model, X_test.T, Y_test.T, W=W_test.T)

    return Test_Results(param_vals, experiment, test_accuracy, x_tst_err, best_model)
                              
def train_test_split(X, Y, **kwargs):
    '''
    Split data into training and testing sets.
    
    Params:
    test_size (optional): fraction of original data to be used for testing, default 0.2
    
    '''
    test_size = kwargs.get('test_size', 0.2)
    random_state = kwargs.get('random_state', 42)
    W = kwargs.get('W', np.ones(X.shape))
    
    X_train, X_test, Y_train, Y_test, W_train, W_test = sk_train_test_split(X, Y, W, test_size=test_size, random_state=random_state)

    return Haddock_Experiment(X_train, Y_train, X_test, Y_test, W_train, W_test)
    
def train(param_vals, experiment, **kwargs):
    '''
    Conduct SSNMF on X_train, Y_train to produce Fulldata training accuracy. 
    Train SSNMF on the whole X_train, Y_train dataset and evaluate it on the X_train, Y_train dataset.
    Use factors A and B from training, conduct SSNMF on X_test and Y_test to produce testing accuracy.
    
    Parameters:
    param_vals (namedtuple): Param(k=, lambda_val=, random_state=)
    N (int, optional): number of iterations to train SSNMF, default 1000
    
    Returns:
    train_score (float): training accuracy score
    param_vals (dict): dictionary, keys=param_name, vals=param_vals
    Xreconerr (float): ||X-AS|| reconstruction error
    Yreconerr (float): ||X-AS|| reconstruction error
    test_score (float): testing accuracy score
    '''
    N = kwargs.get('N', 1000)

    X_train, Y_train, X_test, Y_test, W_train, W_test = experiment.X_train, experiment.Y_train, experiment.X_test, experiment.Y_test, experiment.W_train, experiment.W_test

    full_model = Pypi_SSNMF(X=X_train.T, Y=Y_train.T, W=W_train.T, k=param_vals.k, lam=param_vals.lambda_val, random_state=param_vals.random_state, modelNum=3)

    full_model.mult(numiters=N)
    # Here X_tst_err is the same as get_Xreconerr 
    train_score, X_tst_err =  get_accuracy(full_model, X_train.T, Y_train.T, S=full_model.S, W=W_train.T) 
    X_reconerr = X_tst_err
    Y_reconerr = get_Yreconerr(full_model)

    return Train_Results(train_score, param_vals, X_reconerr, Y_reconerr, experiment)
    
    
def get_model(param_vals, X, Y, **kwargs):
    '''
    Train Pypi_SSNMF on self.X, self.Y dataset (no data is hidden) using the provided parameters
    These parameters produce the highest training accuracy (not necessarily highest testing accuracy).
    Specify alternative parameters in optional argument.
    
    Parameters:
    param_vals (namedtuple): Param(k=, lambda_val=, random_state=). Best parameters found for train
    
    Returns: None
    '''
    N = kwargs.get('N', 1000) 
    
    model = Pypi_SSNMF(X=X.T, Y=Y.T, W=W.T, k=param_vals.k, lam=param_vals.lambda_val, random_state=param_vals.random_state, modelNum=3)

    model.mult(numiters=N)
    return model

def fulldatasearch(experiment, **kwargs):

    '''
    Fulldatasearch is a non-crossvalidated algorithm. For each combination of parameters,
    train on training set, test on testing set. Other than separation of data into training 
    and testing sets, no other separation of data (as with crossvalidation).

    Important distinction between fulldatasearch and gridsearch:
    
    Training accuracy, Fulldatasearch: train the SSNMF on the training data and seeing how well
    it predicts the training labels. 

    Training accuracy, Gridsearch: train the SSNMF on 4/5 of the training data and seeing how well
    it predicts the unseen 1/5 of the training labels. Cross-validation accuracy is like fulldatasearch
    testing accuracy.

    Testing accuracy, Fulldatasearch/Gridsearch: Using SSNMF trained on training data from above, see how
    well it predicts the testing labels. Same for both Fulldatasearch/Gridsearch.

    Default parameter range used from SSNMF.PARAM_RANGE. Specify parameter range
    using param_range. Uses parallel processing for higher speed.

    Parameters:
    param_range (list, optional): parameter range. Default: SSNMF_TYPE.PARAM_RANGE
    experiment (dict): dictionary of X_train, X_test, etc. numpy arrays

    Returns:
    Fulldatasearch_Result (namedtuple): (train_results, test_results, reconerr_results)

    Notes:
    best_accuracy (float): calculated by sklearn accuracy_score
    best_params (dict): dictionary, keys=param_name, vals=best_param_val
    
    train_results (dict): {'best_train_accu': (float), 'best_train_param': (DataFrame), 'train_accu_distr': (DataFrame)}

    test_results (dict): {'best_test_acc': (float), 'best_train_param': (DataFrame), 'test_accu_distr': (DataFrame)}

    reconerr_results (dict): {'Xreconerr_distr': (DataFrame), 'Yreconerr_distr': (DataFrame)}
    
    Notes: 
    train_accu_distr (DataFrame, optional): {'k': [accuracies]}
    test_accu_distr (DataFrame, optional): {'k': [accuracies]}
    Xreconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    Yreconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    '''
    X_train, Y_train, X_test, Y_test, W_train, W_test = experiment.X_train, experiment.Y_train, experiment.X_test, experiment.Y_test, experiment.W_train, experiment.W_test
    param_range = kwargs.get('param_range', PARAM_RANGE)
    param_names = list(param_range.keys())
    param_vals = list(param_range.values())

    
    comb = list(itertools.product(*param_vals)) # all possible combinations of params
    param_keys_and_comb = [Param(*s) for s in comb]
    
    best_accuracy_overall = 0
    best_param_vals_overall = dict()
    
    train_accu_distr = pd.DataFrame()
    test_accu_distr = pd.DataFrame()
    
    Xreconerr_distr = pd.DataFrame()
    Yreconerr_distr = pd.DataFrame()
    
    
    best_train_results = 0
    best_train_overall = 0 
    best_train_param_overall = None

    # results = dict()
    results = []

    for comb in param_keys_and_comb.items():
        trial_results = train(comb, experiment)
        train_score = trial_results.train_score
        results.append(trial_results)

        if best_train_results < train_score:
            best_train_overall = train_score
            best_train_results = trial_results
            best_train_param_overall = k
    
    for k in param_range['k']:
        train_accu = [r.train_score for r in results if r.param_vals.k == k]
        train_accu_distr[k] = pd.Series(train_accu)

        test_accu = [r.test_score for r in results if r.param_vals.k == k]
        test_accu_distr[k] = pd.Series(test_accu)
        
    for k in param_range['k']:
        Xreconerr = [r.xreconerr for r in results if r.param_vals.k == k]
        Xreconerr_distr[k] = pd.Series(Xreconerr)

        Yreconerr = [r.yreconerr for r in results if r.param_vals.k == k]
        Yreconerr_distr[k] = pd.Series(Yreconerr)
    
    fulldata_best_train_param_vals = best_train_param_overall
    fulldata_best_train_model = get_model(fulldata_best_train_param_vals, X_train, Y_train, **kwargs) # model trained on self.X, self.Y, w/ best train params

    train_results = {'best_train_accu': best_train_overall, 'best_train_param': best_train_param_overall, 'train_accu_distr': train_accu_distr}
    test_results = {'test_accu_distr': test_accu_distr}
    reconerr_results = {'Xreconerr_distr': Xreconerr_distr, 'Yreconerr_distr': Yreconerr_distr}
    
    return Fulldatasearch_Results(train_results, test_results, reconerr_results)

def gridsearch(experiment, param_range, **kwargs):
    '''
    Conduct gridsearch with cross-validation over all parameters of the SSNMF. Default
    parameter range used from SSNMF.PARAM_RANGE. Specify parameter range
    using param_range. Uses parallel processing for higher speed.
    
    Parameters:
    random_state (int, optional): inital random seed for cross_validation split
    param_range (list, optional): parameter range. Default: SSNMF_TYPE.PARAM_RANGE
    
    experiment (namedtuple
    
    Returns:
    best_accuracy (float): calculated by sklearn accuracy_score
    best_params (dict): dictionary, keys=param_name, vals=best_param_val
    accu_distr (DataFrame, optional): {'k': [accuracies]}
    Xreconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    Yreconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    X_cvtst_reconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    '''

    folds = kwargs.get('folds', 5)
    random_state = kwargs.get('random_state', 42)
    
    kf = KFold(n_splits=folds, shuffle=True, random_state=random_state)
    
    param_names = list(param_range.keys())
    param_vals = list(param_range.values())

    
    comb = list(itertools.product(*param_vals)) # all possible combinations of params
    param_keys_and_comb = [Param(*s) for s in comb]
    
    best_accuracy_overall = 0
    best_param_vals_overall = dict()
    
    accu_distr = pd.DataFrame()
    Xreconerr_distr = pd.DataFrame()
    Yreconerr_distr = pd.DataFrame()
    X_cvtst_reconerr_distr = pd.DataFrame()
    
    best_accuracy_overall = 0
    best_param_vals_overall = None
    results = []

    for comb in param_keys_and_comb:
        trial_results = cross_validate(comb, experiment, kf, **kwargs)
        val_score = trial_results.validation_score
        results.append(trial_results)
        if val_score > best_accuracy_overall:
            best_accuracy_overall = val_score
            best_param_vals_overall = comb
        
    
    for k in param_range['k']:
        accu = [r.validation_score for r in results if r.param_vals.k == k]
        accu_distr[k] = pd.Series(accu)

    for k in param_range['k']:
        Xreconerr = [r.xreconerr for r in results if r.param_vals.k == k]
        Xreconerr_distr[k] = pd.Series(Xreconerr)

        Yreconerr = [r.yreconerr for r in results if r.param_vals.k == k]
        Yreconerr_distr[k] = pd.Series(Yreconerr)

        X_cvtst_reconerr = [r.x_valreconerr for r in results if r.param_vals.k == k]
        X_cvtst_reconerr_distr[k] = pd.Series(X_cvtst_reconerr)

    best_param_vals = best_param_vals_overall
    return Gridsearch_Result(best_accuracy_overall, best_param_vals_overall, accu_distr, Xreconerr_distr, Yreconerr_distr, X_cvtst_reconerr_distr, experiment)


def unpack_gridsearch(splitted_data, param_range):
    return gridsearch(splitted_data, param_range=param_range)

def unpack_train(gridsearch_results):
    return train(gridsearch_results.best_param_vals_overall, gridsearch_results.experiment)

def unpack_test(gridsearch_results):
    return test(gridsearch_results.best_param_vals_overall, gridsearch_results.experiment)


def haddock_multi_ssnmf(experiments, **kwargs):

    '''
    Uses Python multiprocessing to run a list of SSNMF experiments.

    Parameters:
    experiments (list): list of tuples (X,Y) of data and label matrices

    Returns:
    experiment_results (list): list of Haddcok_Multi_SSNMF results
    '''

    num_cores = cpu_count()

    param_range = kwargs.get('param_range', PARAM_RANGE)
    
    with Pool(num_cores) as pool:
        splitted_data = pool.starmap(train_test_split, experiments) # splitted_data is list of namedtuples Haddock_Experiment(X_train=, Y_train=, etc}

        data_and_params = [(split, param_range) for split in splitted_data]
        gridsearch_results = pool.starmap(gridsearch, data_and_params) # validation accuracy list of namedtuples Gridsearch_Results

        train_results = pool.map(unpack_train, gridsearch_results) # training accuracy on best parameters, list of nametuples Train_Results 
        test_results = pool.map(unpack_test, gridsearch_results)  # testing accuracy on best parameters, list of namedtuples Test_Results 

        results = [Haddock_Multi_SSNMF(i,j,k) for i,j,k in zip(gridsearch_results, train_results, test_results)]
        return results
        