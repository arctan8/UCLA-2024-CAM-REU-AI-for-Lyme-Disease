from multiprocessing import Pool, cpu_count
import pypi_ssnmf
import numpy as np
import pandas as pd

from pypi_ssnmf import Pypi_SSNMF

from sklearn.model_selection import train_test_split as sk_train_test_split

from scipy.optimize import lsq_linear
from sklearn.metrics import accuracy_score

X_TRAIN = 'X_train'; Y_TRAIN = 'Y_train'; W_TRAIN = 'W_train'
X_TEST = 'X_test'; Y_TEST = 'Y_test'; W_TEST = 'W_test'

TRAIN_SCORE = 'train_score'; PARAM_VALS = 'param_vals'
X_RECONERR = 'X_reconerr'; Y_RECONERR = 'Y_reconerr'; TEST_SCORE = 'test_score'

VALIDATION_SCORE = 'validation_score'; X_VALIDATION_RECONERR = 'X_validation_reconerr'

BEST_ACCURACY_OVERALL = "best_accuracy_overall"
BEST_PARAM_VALS_OVERALL = "best_param_vals_overall"
ACCU_DISTR = "accu_distr"
XRECONERR_DISTR = "Xreconerr_distr"
YRECONERR_DISTR = "Yreconerr_distr"
X_CVTST_RECONERR_DISTR = "X_cvtst_reconerr_distr"

TRAIN_RESULTS = "train_results"
TEST_RESULTS = "test_results"
RECONERR_RESULTS = "reconerr_results"

EXPERIMENT = "experiment"

TEST_ACCURACY = "test_accuracy"
X_TEST_ERROR= "x_test_error"
PARAM_RANGE = {'k': range(2,7),'lambda': list(np.linspace(0,1,10)), 'random_state': range(0,10)}

def get_Xreconerr(model):
        '''
        Given a trained model, compute reconstruction error ||X-AS||_F,
        using frobenius norm.

        Parameters:
        model (Pypi_SSNMF): trained model

        Return:
        reconerr (float): reconstruction error
        '''
        
        return np.linalg.norm(np.multiply(model.W, model.X - model.A @ model.S), ord='fro')

def get_Yreconerr(model):
        '''
        Given a trained model, compute reconstruction error ||Y-BS||_F,
        using frobenius norm.

        Parameters:
        model (Pypi_SSNMF): trained model

        Return:
        reconerr (float): reconstruction error
        '''
        
        return np.linalg.norm(np.multiply(model.L, model.Y - model.B @ model.S), ord='fro')

def find_matrix_S(X, A, **kwargs):
        '''
        Compute coefficient matrix S from data matrix X and basis matrix A using 
        Nonnegative Least Squares: X approx. AS.
        
        Parameters:
        X (np.array): data matrix, cases x features
        A (np.array): basis matrix, cases x topics
        W (np.array, optional): weight matrix, cases x features
        
        Returns:
        S (np.array): coefficient matrix, topics x features
        tol (float, optional): NNLS tolerance, default 1e-10
        '''
        if X.shape[0] != A.shape[0]:
            raise Exception('Shape mismatch, X: ',X.shape,' A: ',A.shape)
        
        W = kwargs.get('W', np.ones(X.shape))
        tol = kwargs.get('tol',1e-10)
        
        num_samples, num_features = X.shape
        num_components = A.shape[1]
        
        S = np.zeros((num_components, num_features))
        ### Decrease precision to avoid NNLS Non-convergence? Or reduce # of iterations
        
        for i in range(num_features):
            s_i = None
            if W is None:
                nnls_result = lsq_linear(A, X[:,i], bounds=(0,np.inf), tol=tol)
                s_i = nnls_result.x

            else:
                W_i = W[:,i]
                W_i_matrix = np.diag(W_i)
                X_i = X[:,i]
                
                nnls_result = lsq_linear(W_i_matrix@A, W_i_matrix@X_i, bounds=(0, np.inf), tol=tol)
                s_i = nnls_result.x
            S[:, i] = s_i
            
        return S

def get_accuracy(model, X_data, Y_labels, A, **kwargs):
        '''
        Given a trained model and actual label matrix,
        calculate accuracy by comparing the predicted labels to the 
        actual labels.
        
        Parameters:
        model (Pypi_SSNMF): trained model
        X_data (np.array): data to predict labels
        Y_labels (np.array): actual label matrix (rows=patients, col=label)
        S (np.array, optional): coefficient matrix, specified for fulldata_validation or calculated via NNLS if unspecified.
        A (np.array): matrix A
        W (np.array, optional): weight matrix W
        
        Returns:
        accuracy (float): accuracy score
        X_tst_err (float): ||X - AS|| for S generated by NNLS
        '''
        A = kwargs.get('A', model.A)
        B = model.B
       
        W = kwargs.get('W', np.ones(X_data.shape))
        
        S = kwargs.get('S', find_matrix_S(X_data, A, W=W))
        X_tst_err = np.linalg.norm(X_data - A@S, ord='fro')
        
        Y_hat = B @ S
    
        if Y_labels.ndim == 1:
            Y_labels = np.column_stack((Y_labels, 1-Y_labels)) # Make Y_labels 2D
        Y_labels = Y_labels.T
        
        Y_hat_labels = np.round(Y_hat)
        Y_hat_labels = Y_hat_labels.T        
        y_len = len(Y_hat_labels)
        
        assert Y_hat_labels.shape == Y_labels.shape
        
        correct_pred = 0 # True positive + True negative
        
        for true_label, pred_label in zip(Y_labels, Y_hat_labels):
            if (true_label == pred_label).all():
                correct_pred += 1
        
        accuracy = correct_pred / y_len # (TP + TN)/(TP+TN+FP+FN)
        return accuracy, X_tst_err
    
def cross_validate(param_values, experiment, kf, **kwargs):

    '''
    Compute the accuracy using cross-validation on Pypi_SSNMF
    with specified parameters.
    
    Parameters:
    kf (KFold): KFold object to split training data into folds
    X_train (np.array): numpy array of training data
    Y_train (np.array): numpy array of training labels
    W_train (np.array, optional): numpy array of training mask. Same shape as X_train. Default all 1's.
    param_values (dict): dictionary, keys=param_name, vals=param_vals
    N (int, optional): number of iterations to train SSNMF, default 1000
    
    
    Returns:
    avg_score (float): accuracy score, averaged over all folds
    param_values (dict): dictionary, keys=param_name, vals=param_vals
    avg_Xreconerr (float): ||X-AS|| reconstruction error, averaged over all folds
    avg_Yreconerr (float): ||X-AS|| reconstruction error, averaged over all folds
    avg_X_tst_err (float): ||X_cv_tst -A S_nnls|| reconstruction error, averaged over all folds
    '''
    
    N = kwargs.get('N', 1000)
    W_train = kwargs.get('W_train', np.ones(X_train.shape))
    
    X_train, Y_train, X_test, Y_test, W_train, W_test = experiment[X_TRAIN], experiment[Y_TRAIN], experiment[X_TEST], experiment[Y_TEST], experiment[W_TRAIN], experiment[W_TEST]

    scores = []
    X_errs = []
    Y_errs = []
    X_tst_errs = []
    
    for train_index, val_index in kf.split(X_train):
        X_train_cv, X_val_cv = X_train[train_index, :].T, X_train[val_index, :].T
        Y_train_cv, Y_val_cv = Y_train[train_index, :].T, Y_train[val_index, :].T
        W_train_cv, W_val_cv = W_train[train_index, :].T, W_train[val_index, :].T

        # X_train_cv: features x patients
        # Y_train_cv: features x labels
            
        model = Pypi_SSNMF(X=X_train_cv, Y=Y_train_cv, W=W_train_cv, k=param_values['k'], lam=param_values['lambda'], random_state=param_values['random_state'], modelNum=3)
        
        
        model.mult(numiters=N)

        accuracy, X_tst_err = get_accuracy(model,X_val_cv,Y_val_cv) 
        scores.append(accuracy)
        X_tst_errs.append(X_tst_err)

        X_err = get_Xreconerr(model)
        X_errs.append(X_err)

        Y_err = get_Yreconerr(model)
        Y_errs.append(Y_err)
        
    avg_score = np.mean(scores)
    avg_X_reconerr = np.mean(X_errs)
    avg_Y_reconerr = np.mean(Y_errs)
    avg_X_tst_err = np.mean(X_tst_errs)
    
    return {VALIDATION_SCORE: avg_score, PARAM_VALUES: param_values, X_RECONERR: avg_X_reconerr, Y_RECONERR: avg_Y_reconerr, X_VALIDATION_RECONERR: avg_X_tst_err}

def test(param_vals, experiment, *args, **kwargs):
    '''
    Using provided parameter values calculated by gridsearch or specified values, train SSNMF on full training set, set self.best_model, then tests it.
    
    Parameters:
    param_vals (dict): keys=param_names, values=param_vals
    experiment (dict): dictionary of numpy arrays X_train, X_test, etc.
    
    Returns:
    accuracy (float): calculated by sklearn.accuracy_score
    X_tst_err (float): ||X_tst - A S_nnls ||
    '''
    N = kwargs.get('N', 1000)
    
    X_train, Y_train, X_test, Y_test, W_train, W_test = experiment[X_TRAIN], experiment[Y_TRAIN], experiment[X_TEST], experiment[Y_TEST], experiment[W_TRAIN], experiment[W_TEST]
    if param_vals is None:
        raise Exception('Call Gridsearch before testing!')
    
    W_train = np.ones(X_train.shape)
        
    best_model = Pypi_SSNMF(X=X_train.T, Y=Y_train.T, W=W_train.T, k=param_vals['k'], lam=param_vals['lambda'], random_state=param_vals['random_state'], modelNum=3)
    
    best_model.mult(numiters=N)

    test_accuracy, x_tst_err = get_accuracy(best_model, X_test.T, Y_test.T, W=W_test.T) 
    return {PARAM_VALS: param_vals, EXPERIMENT: experiment, TEST_ACCURACY: test_accuracy, X_TEST_ERROR: x_tst_err}

def train_test_split(X, Y, **kwargs):
    '''
    Split data into training and testing sets.
    
    Params:
    test_size (optional): fraction of original data to be used for testing, default 0.2
    random_state (optional): random seed, default 42
    
    '''
    test_size = kwargs.get('test_size', 0.2)
    random_state = kwargs.get('random_state', 42)
    W = kwargs.get('W', np.ones(X.shape))
    
    X_train, X_test, Y_train, Y_test, W_train, W_test = sk_train_test_split(X, Y, W, test_size=test_size, random_state=random_state)

    return {X_TRAIN: X_train, Y_TRAIN: Y_train, W_TRAIN: W_train, X_TEST: X_test, Y_TEST: Y_test, W_TEST: W_test}
    
def fulldata_validate(param_vals, experiment, **kwargs):
    '''
    SSNMF on X_train, Y_train to produce Fulldata training accuracy. 
    Use factors A and B from training, conduct SSNMF on X_test and Y_test to produce testing accuracy.
    
    Parameters:
    param_values (dict): dictionary, keys=param_name, vals=param_vals
    N (int, optional): number of iterations to train SSNMF, default 1000
    
    Returns:
    train_score (float): training accuracy score
    param_values (dict): dictionary, keys=param_name, vals=param_vals
    Xreconerr (float): ||X-AS|| reconstruction error
    Yreconerr (float): ||X-AS|| reconstruction error
    test_score (float): testing accuracy score
    '''
    N = kwargs.get('N', 1000)
    W_train = kwargs.get('W_train', np.ones(X_train.shape))

    X_train, Y_train, X_test, Y_test, W_train, W_test = experiment[X_TRAIN], experiment[Y_TRAIN], experiment[X_TEST], experiment[Y_TEST], experiment[W_TRAIN], experiment[W_TEST]

    full_model = Pypi_SSNMF(X=X_train.T, Y=Y_train.T, W=W_train.T, k=param_vals['k'], lam=param_vals['lambda'], random_state=param_vals['random_state'], modelNum=3)

    full_model.mult(numiters=N)
    # Here X_tst_err is the same as get_Xreconerr 
    train_score, X_tst_err =  get_accuracy(full_model, X_train.T, Y_train.T, S=full_model.S, W=W_train.T) 
    X_reconerr = X_tst_err
    Y_reconerr = get_Yreconerr(full_model)

    test_score, _ = get_accuracy(full_model, X_test.T, Y_test.T, W=W_test.T)

    return {TRAIN_SCORE: train_score, PARAM_VALS: param_vals, X_RECONERR: X_reconerr, Y_RECONERR: Y_reconerr, TEST_SCORE: test_score, EXPERIMENT: experiment}
    
def get_model(param_vals, X, Y, **kwargs):
    '''
    Train Pypi_SSNMF on self.X, self.Y dataset (no data is hidden) using the provided parameters
    These parameters produce the highest training accuracy (not necessarily highest testing accuracy).
    Specify alternative parameters in optional argument.
    
    Parameters:
    _param_vals (dict, optional): keys=param_names, values=param_vals. Best parameters found for fulldata_validate
    
    Returns: None
    '''
    N = kwargs.get('N', 1000) 
    
    model = Pypi_SSNMF(X=X.T, Y=Y.T, W=W.T, k=param_vals['k'], lam=param_vals['lambda'], random_state=param_vals['random_state'], modelNum=3)

    model.mult(numiters=N)
    return model

def fulldatasearch(experiment, **kwargs):

    '''
    Fulldatasearch is a non-crossvalidated algorithm. For each combination of parameters,
    train on training set, test on testing set. Other than separation of data into training 
    and testing sets, no other separation of data (as with crossvalidation).

    Important distinction between fulldatasearch and gridsearch:
    
    Training accuracy, Fulldatasearch: train the SSNMF on the training data and seeing how well
    it predicts the training labels. 

    Training accuracy, Gridsearch: train the SSNMF on 4/5 of the training data and seeing how well
    it predicts the unseen 1/5 of the training labels. Cross-validation accuracy is like fulldatasearch
    testing accuracy.

    Testing accuracy, Fulldatasearch/Gridsearch: Using SSNMF trained on training data from above, see how
    well it predicts the testing labels. Same for both Fulldatasearch/Gridsearch.

    Default parameter range used from SSNMF.PARAM_RANGE. Specify parameter range
    using param_range. Uses parallel processing for higher speed.

    Parameters:
    param_range (list, optional): parameter range. Default: SSNMF_TYPE.PARAM_RANGE
    experiment (dict): dictionary of X_train, X_test, etc. numpy arrays
    get_topic_accu_distr (bool, optional): if True, returns DataFrame of accuracies. Default False
    get_reconerr_distr (bool, optional): if True, returns DataFrame of reconstruction errors. Default False.

    Returns:
    best_accuracy (float): calculated by sklearn accuracy_score
    best_params (dict): dictionary, keys=param_name, vals=best_param_val

    train_results (dict): {'best_train_accu': (float), 'best_train_param': (DataFrame), 'train_accu_distr': (DataFrame)}

    test_results (dict): {'best_test_acc': (float), 'best_train_param': (DataFrame), 'test_accu_distr': (DataFrame)}

    reconerr_results (dict): {'Xreconerr_distr': (DataFrame), 'Yreconerr_distr': (DataFrame)}
    
    Notes: 
    train_accu_distr (DataFrame, optional): {'k': [accuracies]}
    test_accu_distr (DataFrame, optional): {'k': [accuracies]}
    Xreconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    Yreconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    '''
    X_train, Y_train, X_test, Y_test, W_train, W_test = experiment[X_TRAIN], experiment[Y_TRAIN], experiment[X_TEST], experiment[Y_TEST], experiment[W_TRAIN], experiment[W_TEST]
    param_range = kwargs.get('param_range', PARAM_RANGE)
    param_names = list(param_range.keys())
    param_vals = list(param_range.values())

    get_topic_accu_distr = kwargs.get('get_topic_accu_distr', False)
    get_reconerr_distr = kwargs.get('get_reconerr_distr', False)
    
    comb = list(itertools.product(*param_vals)) # all possible combinations of params
    param_keys_and_comb = [dict(zip(param_names, s)) for s in comb]
    
    best_accuracy_overall = 0
    best_param_vals_overall = dict()
    
    train_accu_distr = pd.DataFrame()
    test_accu_distr = pd.DataFrame()
    
    Xreconerr_distr = pd.DataFrame()
    Yreconerr_distr = pd.DataFrame()
    
    # Define partial function to call self.get_accuracy with the same kFold object kf
    # partial_func = partial(self.fulldata_validate,**kwargs) 
    
    # with Pool(num_cores) as pool:
    # pool.map returns a long list of tuples (accuracy_score, {param: vals}, X_reconerr, Y_reconerr, test_accuracy_score)
    # results = pool.map(partial_func, param_keys_and_comb)
    
    best_train_results = 0
    best_train_overall = 0 
    best_train_param_overall = None

    results = dict()

    for comb in param_keys_and_comb.items():
        trial_results = fulldata_validate(comb, X_train, Y_train, X_test, Y_test)
        train_score = trial_results[TRAIN_SCORE]
        results[comb] =  trial_results

        if best_train_results < train_score:
            best_train_overall = train_score
            best_train_results = trial_results
            best_train_param_overall = k

    
    print('best train results: ', best_train_results)
    
    if get_topic_accu_distr:
        for k in param_range['k']:
            train_accu = [r[TRAIN_SCORE] for r in results if r[PARAM_VALS]['k'] == k]
            train_accu_distr[k] = pd.Series(train_accu)

            test_accu = [r[TEST_SCORE] for r in results if r[PARAM_VALS]['k'] == k]
            test_accu_distr[k] = pd.Series(test_accu)

    if get_reconerr_distr:
        for k in param_range['k']:
            Xreconerr = [r[X_RECONERR] for r in results if r[PARAM_VALS]['k'] == k]
            Xreconerr_distr[k] = pd.Series(Xreconerr)

            Yreconerr = [r[Y_RECONERR] for r in results if r[PARAM_VALS]['k'] == k]
            Yreconerr_distr[k] = pd.Series(Yreconerr)
    
    fulldata_best_train_param_vals = best_train_param_overall
    fulldata_best_train_model = get_model(fulldata_best_train_param_vals, X_train, Y_train, **kwargs) # model trained on self.X, self.Y, w/ best train params

    train_results = {'best_train_accu': best_train_overall, 'best_train_param': best_train_param_overall, 'train_accu_distr': train_accu_distr}

    reconerr_results = {'Xreconerr_distr': Xreconerr_distr, 'Yreconerr_distr': Yreconerr_distr}
    
    return {TRAIN_RESULTS: train_results, RECONERR_RESULTS: reconerr_results}
    

def gridsearch(experiment, **kwargs):
    '''
    Conduct gridsearch with cross-validation over all parameters of the SSNMF. Default
    parameter range used from SSNMF.PARAM_RANGE. Specify parameter range
    using param_range. Uses parallel processing for higher speed.
    
    Parameters:
    random_state (int, optional): inital random seed for cross_validation split
    param_range (list, optional): parameter range. Default: SSNMF_TYPE.PARAM_RANGE
    
    experiment: dictionary containing {X}
    
    Returns:
    best_accuracy (float): calculated by sklearn accuracy_score
    best_params (dict): dictionary, keys=param_name, vals=best_param_val
    accu_distr (DataFrame, optional): {'k': [accuracies]}
    Xreconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    Yreconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    X_cvtst_reconerr_distr (DataFrame, optional): {'k': [reconerrs]}
    '''
    X_train = experiment[X_TRAIN]
    Y_train = experiment[Y_TRAIN]

    folds = kwargs.get('folds', 5)
    random_state = kwargs.get('random_state', 42)
    
    kf = KFold(n_splits=folds, shuffle=True, random_state=random_state)
    
    param_range = kwargs.get('param_range', PARAM_RANGE)
    param_names = list(param_range.keys())
    param_vals = list(param_range.values())

    get_topic_accu_distr = kwargs.get('get_topic_accu_distr', False)
    get_reconerr_distr = kwargs.get('get_reconerr_distr', False)
    
    comb = list(itertools.product(*param_vals)) # all possible combinations of params
    param_keys_and_comb = [dict(zip(param_names, s)) for s in comb]
    
    # num_cores = cpu_count()
    
    best_accuracy_overall = 0
    best_param_vals_overall = dict()
    
    accu_distr = pd.DataFrame()
    Xreconerr_distr = pd.DataFrame()
    Yreconerr_distr = pd.DataFrame()
    X_cvtst_reconerr_distr = pd.DataFrame()
    
    # Define partial function to call self.get_accuracy with the same kFold object kf
    # partial_func = partial(self.cross_validate, kf=kf, **kwargs)
    
    # with Pool(num_cores) as pool:
        # pool.map returns a long list of tuples (accuracy_score, {param: vals})
        # results = pool.map(partial_func, param_keys_and_comb)    
        # best_result = max(results, key=lambda x: x[0])
    
    best_accuracy_overall = 0
    best_param_vals_overall = None
    results = dict()

    for comb in param_keys_and_comb:
        trial_result = cross_validate(X_train, Y_train, comb, kf, **kwargs)
        valid_score = trial_results[VALIDATION_SCORE]
        results[comb] = trial_result
        if val_score > best_accuracy_overall:
            valid_score = best_accuracy_overall
            best_param_vals_overall = comb
        
    
    for k in param_range['k']:
        accu = [r[VALIDATION_SCORE] for r in results if r[PARAM_VALS]['k'] == k]
        accu_distr[k] = pd.Series(accu)

    for k in param_range['k']:
        Xreconerr = [r[X_RECONERR] for r in results if r[PARAM_VALS]['k'] == k]
        Xreconerr_distr[k] = pd.Series(Xreconerr)

        Yreconerr = [r[Y_RECONERR] for r in results if r[PARAM_VALS]['k'] == k]
        Yreconerr_distr[k] = pd.Series(Yreconerr)

        X_cvtst_reconerr = [r[X_VALIDATION_RECONERR] for r in results if r[PARAM_VALS]['k'] == k]
        X_cvtst_reconerr_distr[k] = pd.Series(X_cvtst_reconerr)

    best_param_vals = best_param_vals_overall
    return {
            BEST_ACCURACY_OVERALL: best_accuracy_overall,
            BEST_PARAM_VALS_OVERALL: best_param_vals_overall,
            ACCU_DISTR: accu_distr,
            XRECONERR_DISTR: Xreconerr_distr,
            YRECONERR_DISTR: Yreconerr_distr,
            X_CVTST_RECONERR_DISTR: X_cvtst_reconerr_distr,
            EXPERIMENT: experiment
            }
    

def unpack_gridsearch(splitted_data):
    return gridsearch(splitted_data[X_TRAIN], splitted_data[Y_TRAIN])

def unpack_fulldata_validate(gridsearch_results):
    
    return fulldata_validate(gridsearch_results[BEST_PARAM_VALS_OVERALL], gridsearch_results[EXPERIMENT])

def unpack_test(gridsearch_results):
    return test(gridsearch_results[BEST_PARAM_VALS_OVERALL], gridsearch_results[EXPERIMENT])

def multi_haddock_ssnmf(experiments):

    '''
    Uses Python multiprocessing to run a list of SSNMF experiments.

    Parameters:
    experiments (list): list of tuples (X,Y) of data and label matrices

    Returns:
    experiment_results (list): list of experimental results
    '''

    num_cores = cpu_count()

    with Pool(num_cores) as pool:
        splitted_data = pool.starmap(train_test_split, experiments) # splitted_data is list of dictionaries {X_TRAIN:x_train, X_TEST:x_test ...}
        gridsearch_results = pool.map(unpack_gridsearch, splitted_data) # validation accuracy list of dictionaries {BEST_ACCURACY_OVERALL:, BEST_PARAM _VALS_OVERALL ...}
        train_results = pool.map(unpack_fulldata_validate, gridsearch_results) # training accuracy on best parameters, list of dictionaries {TRAIN_SCORE:, PARAM_VALS }
        test_results = pool.map(unpack_test, gridsearch_results)  # testing accuracy on best parameters, list of dictionaries {PARAM_VALS: param_vals, EXPERIMENT: experiment, TEST_ACCURACY: test_accuracy, X_TEST_ERROR: x_tst_err}

        for validate,train,test in gridsearch_results, train_results, test_results:
            print(f'Best Validation Accuracy: {validate[BEST_ACCURACY_OVERALL]}')
            print(f'Best Parameter Values: {validate[BEST_PARAM_VALS_OVERALL]}')
            # TODO VISUALS
            print(f'Training Accuracy: {train[TRAIN_RESULTS]}')
            print(f'Test Results: {test[TEST_RESULTS]}')


        
